\section{Symplectic Model Reduction} \label{chap:SyMo:1}
We now {\edit discuss} how to modify reduced order modeling to ensure that {\edit the resulting scheme preserves} the symplectic structure of the Hamiltonian system. 



Consider a Hamiltonian system (\ref{eq:Hasy:7}) on a $2n$-dimensional symplectic vector space $\edit (V,\Omega)$. Suppose that the solution manifold $\mathcal M_H$ is well approximated by a low dimensional symplectic subspace $\edit (W,\Omega)$ of dimension $2k$ $(k\ll n)$. We can {\edit then} construct a symplectic basis $A$ for $\edit W$ and approximate the solution to (\ref{eq:Hasy:7}) as
\begin{equation} \label{eq:SyMo:1}
	\mathbf z \approx A\mathbf y.
\end{equation}
Substituting this into (\ref{eq:Hasy:7}) we obtain
\begin{equation} \label{eq:SyMo:2}
	A\mathbf y = \mathbb{J}_{2n} \nabla_{\mathbf z} H(A \mathbf y). 
\end{equation}
Multiplying both sides with the symplectic inverse of $A$ and using the chain rule we have
\begin{equation} \label{eq:SyMo:3}
	\mathbf y = A^+ \mathbb J_{2n} (A^+)^T \nabla_{\mathbf y} H(A\mathbf y).
\end{equation}
Since $A$ is a symplectic basis, Lemma \ref{lemma:Hasy:1} ensures that $(A^+)^T$ is a symplectic matrix i.e., $A^+ \mathbb J_{2n} (A^+)^T = \mathbb{J}_{2k}$. By defining the reduced Hamiltonian $\tilde H:\mathbb R^{2k} \to \mathbb R$ as $\tilde H (y) = H(Ay)$ we obtain the reduced system
\begin{equation} \label{eq:SyMo:4}
\left\{
\begin{aligned}
	 \frac{d}{dt} \mathbf y &= \mathbb J_{2k} \nabla_{\mathbf y} \tilde H(\mathbf y), \\
	 \mathbf y_0 &= A^+ \mathbf z_0.
\end{aligned}
\right.
\end{equation}
The system obtained from the Petrov-Galerkin projection in (\ref{eq:MoOr:5}) is not a Hamiltonian system and does not guarantee conservation of the symplectic structure. On the other hand, we observe that the reduced system in (\ref{eq:SyMo:4}) is of the form (\ref{eq:Hasy:7}) and, hence, is a Hamiltonian system, i.e. the symplectic structure will be conserved along integral curves of (\ref{eq:SyMo:4}). Note that the original and the reduced systems are {\edit endowed with} different Hamiltonians. In the next proposition we show that the error in the Hamiltonian is constant in time. 


\begin{proposition}
Let $\mathbf{z} (t)$ be the solution of (\ref{eq:Hasy:7}) at time $t$. Further suppose that $\tilde{\mathbf{z}} (t)$ is the approximate solution of the reduced system (\ref{eq:SyMo:4}) in the original coordinate system. Then the error in the Hamiltonian defined by
\begin{equation} \label{eq:SyMo:5}
	\Delta H(t)  = |H(\mathbf z(t)) - H(\tilde{\mathbf z}(t))|,
\end{equation}
is constant for all $t\in \mathbb R$.
\end{proposition}

\begin{proof}
	Let $\phi_t$ and $\psi_t$ be the Hamiltonian flow of the original and the reduced system respectively. By definition $\mathbf z(t) = \phi_t(\mathbf z_0)$ and $\mathbf y(t) = \psi_t(\mathbf y_0)$. Using the definition of the reduced Hamiltonian and Theorem \ref{theorem:Hasy:1} we have
\begin{equation} \label{eq:SyMo:6}
\begin{aligned}
	H(\tilde{\mathbf{z}} (t)) = H( A\mathbf y (t) ) = \tilde H(\mathbf y (t)) = \tilde H(\psi_t(\mathbf y_0)) = \tilde H(\mathbf y_0) = \tilde H(A^+ \mathbf z_0) = H(AA^+\mathbf z_0).
\end{aligned}
\end{equation}
The error in the Hamiltonian can then be written in terms of $\mathbf z_0$ and the symplectic basis $A$ as
\begin{equation} \label{eq:SyMo:7}
	\Delta H(t) = |H(\mathbf z_0) - H(AA^+\mathbf z_0)|
\end{equation}
\end{proof}

{\edit 
The following theorems provide a strong indication of the stability of the reduced system. 

\begin{definition} \label{definition:SyMo:1} \cite{bhatia2002stability}
Consider a dynamical system of the form $\dot{\mathbf z} = \mathbf f(\mathbf z)$ and suppose that $\mathbf z_e$ is an equilibrium point for the system so that $\mathbf f(\mathbf z_e) = 0$. $\mathbf z_e$ is called nonlinearly stable or Lyapunov stable if, for any $\epsilon > 0$, we can find $\delta > 0$ such that for any trajectory $\phi_t$, if $\| \phi_0 - \mathbf z_e \|_2 \leq \delta$, then for all $0 \leq t < \infty$, we have $\| \phi_t - \mathbf z_e \|_2 < \epsilon$, where $\| \cdot \|_2$ is the Euclidean norm.
\end{definition}	
The following proposition, also known as Dirichlet's theorem \cite{bhatia2002stability}, states the sufficient condition for an equilibrium point to be Lyapunov stable. We refer the reader to \cite{bhatia2002stability} for the proof.
\begin{proposition} \label{proposition:SyMo:1} \cite{bhatia2002stability}
An equilibrium point $\mathbf z_e$ is Lyapunov stable if there exists a scalar function $W : \mathbb R^{n} \to  \mathbb R$ such that $\nabla W(\mathbf z_e) = 0$, $\nabla^2 W(\mathbf z_e)$ is positive definite, and that for any trajectory $\phi_t$ defined in the neighborhood of $\mathbf z_e$, we have $\frac{d}{dt} W(\phi_t) \leq 0$. Here $\nabla^2W$ is the Hessian matrix of $W$.
\end{proposition}
The scalar function $W$ is referred to as the \emph{Lyapunov function}. In the context of the Hamiltonian systems, a suitable candidate for the Lyapunov function is the Hamiltonian function $H$. The following theorem shows that when $H$ (or $-H$) is a Lyapunov function, then the equilibrium points of the original and the reduced system are Lyapunov stable \cite{abraham1978foundations}. 
\begin{theorem} \label{theorem:SyMo:1}
Consider a Hamiltonian system of the form (\ref{eq:Hasy:7}) together with the reduced system (\ref{eq:SyMo:4}). Suppose $\mathbf z_e$ is an equilibrium point for (\ref{eq:Hasy:7}) and that $\mathbf y_e = A^+\mathbf z_e$. If $H$ (or $-H$) is a Lyapunov function satisfying Proposition \ref{proposition:SyMo:1}, then $\mathbf z_e$ and $\mathbf y_e$ are Lyapunov stable equilibrium points for (\ref{eq:Hasy:7}) and (\ref{eq:SyMo:4}), respectively. 
\end{theorem}
\begin{proof}
	It is a direct consequence of Proposition \ref{proposition:SyMo:1} that $\mathbf z_e$ is a local minimum or maximum of (\ref{eq:Hasy:7}) and also a Lyapunov stable point. It can be easily checked that if $\mathbf z_e$ is a local minimum of $H$ then $\mathbf y_e$ is a local minimum for $\tilde H$ and an equilibrium point for (\ref{eq:SyMo:4}). Also from the chain rule we have
\[
	\nabla^2_{\mathbf y} \tilde H = A^T \nabla^2_{\mathbf z} H A.
\]
So for any $\xi\in \mathbb R^{2k}$
\[
	\xi^T \nabla^2_{\mathbf y} \tilde H \xi = (A\xi)^T \nabla^2_{\mathbf z} H (A\xi) \geq 0.
\]
Here the last inequality is due to the positive definiteness of $H$. Therefore $\tilde H$ is also positive definite. By Proposition \ref{proposition:SyMo:1} we conclude that $\mathbf y_e$ is a Lyapunov stable point.
\end{proof}
}

While the symplectic structure is not guaranteed to be preserved in the reduced systems obtained by the Petrov-Galerkin projection, the reduced system obtained by the symplectic projection guarantees the preservation of the energy up to the error in the Hamiltonian (\ref{eq:SyMo:5}). In the next section we discuss  different methods for obtaining a symplectic basis.

\subsection{Proper Symplectic Decomposition (PSD)} \label{chap:SyMo.PrSy:1}

Similar to Section \ref{chap:MoOr.PrOr:1} we gather snapshots $\mathbf z_i = [q_i^T , p_i^T]^T$ in the snapshot matrix $S$. Suppose that a symplectic basis $A$ of size $2n\times2k$ and its symplectic inverse $A^+$ is provided. {\edit The Proper Symplectic Decomposition} requires that the error of the symplectic projection onto the symplectic subspace {\edit be minimized}. Hence, the PSD symplectic basis of size $2k$ is the solution to the optimization problem

\begin{equation} \label{eq:SyMo:8}
\begin{aligned}
& \underset{V\in \mathbb R^{2n\times 2k}}{\text{minimize}}
& & \| S - AA^+S\|_F \\
& \text{subject to}
& & A^T \mathbb{J}_{2n}A = \mathbb{J}_{2k}
\end{aligned}
\end{equation}
Compared to POD, in (\ref{eq:SyMo:8}) the orthogonal projection is replaced with a symplectic projection $AA^+$. At first, the minimization looks similar to the one obtained by POD. {\edit However, it is well known that symplectic bases are not generally orthogonal, and therefore not norm bounded. This means that numerical errors may become dominant in the symplectic projection \cite{Karow:2006cf} which makes the minimization (\ref{eq:SyMo:8}) a harder problem than (\ref{eq:MoOr:6}).}
	
As the optimization problem (\ref{eq:SyMo:8}) is nonlinear, the direct solution is usually expensive. A simplified version of the optimization (\ref{eq:SyMo:8}) can be found in \cite{Peng:2014di}, but there is no guarantee that the method provides a near optimal basis. 

{\edit Finding eigen-spaces of Hamiltonian and symplectic matrices is studied in the context of optimal control problems \cite{Benner:2000ww,Benner:1997ef,Watkins:2004kz,BunseGerstner:1986dg} and model reduction of Riccati equations \cite{Benner:1997ef}, where also an SVD-like decomposition for Hamiltonian and symplectic matrices has been proposed \cite{Xu:2003kx}. However, the computation of a large snapshot matrix and use of the mentioned methods to compute its eigen-spaces, is usually computationally demanding. Also, these methods generally do not guarantee the construction of a well-conditioned symplectic basis.
	
The greedy approach presented in Section \ref{Chap:Symo.PrSy:3} is an iterative method for construction of a symplectic basis. It avoids the evaluation of the full snapshot matrix, hence substantially reduces the computational cost in the offline stage of the symplectic model reduction. Also, by construction, it yields a orthosymplectic basis and therefore a well-conditioned basis.

In Section \ref{chap:SyMo.PrSy:2} we briefly outline non-direct methods for finding solutions to (\ref{eq:SyMo:8}), proposed by \cite{Peng:2014di}, and assuming a specific structure for $A$. In Section \ref{Chap:Symo.PrSy:3} we introduce a greedy approach for the symplectic basis generation.}

\subsubsection{SVD Based Methods for Symplectic Basis Generation} \label{chap:SyMo.PrSy:2} 


\paragraph{\bf Cotangent lift} Suppose that $A$ is of the form
\begin{equation} \label{eq:SyMo:9}
	A = 
	\begin{pmatrix}
		\Phi & 0 \\
		0 & \Phi
	\end{pmatrix},
\end{equation}
where $\Phi \in \mathbb{R}^{n\times k}$ is an orthonormal matrix. It is easy to check that $A$ is a symplectic matrix, i.e., $A^T \mathbb J_{2n} A = \mathbb J_{2k}$. The construction of $A$ suggests that the range of $\Phi$ should cover both the potential and the momentum spaces. Hence, we can construct $A$ by forming the combined snapshot matrix
\begin{equation} \label{eq:SyMo:10}
	S_{\text{combined}} = [q_1,\dots,q_n,p_1,\dots,p_n], \qquad \mathbf z_i = (q_i^T,p_i^T)^T,
\end{equation}
and define $\Phi=[u_1,\dots,u_k]$, where $u_i$ is the $i$-th left singular vector of $S_{\text{combined}}$. It is shown in \cite{Peng:2014di} that among all symplectic bases of the form (\ref{eq:SyMo:9}) cotangent lift minimizes the projection error.


\paragraph{\bf Complex SVD} Suppose instead that $A$ takes the form \cite{Peng:2014di}
\begin{equation} \label{eq:SyMo:11}
	A = 
	\begin{pmatrix}
		\Phi & -\Psi \\
		\Psi & \Phi
	\end{pmatrix},
\end{equation}
while $\Phi$ and $\Psi$ are real matrices of size $n\times k$ satisfying conditions
\begin{equation} \label{eq:SyMo:12}
\Phi^T \Phi + \Psi^T \Psi = I_k,\quad \Phi^T \Psi = \Psi^T \Phi.
\end{equation}
It can be checked that $A$ forms a symplectic matrix. To construct $A$ we first define the complex snapshot matrix
\begin{equation} \label{eq:SyMo:13}
	S_{\text{complex}} = [ q_1 + i p_1, \dots , q_N + i p_N ].
\end{equation}
Each left singular vector of $S_{\text{complex}}$ now takes the form $u_m = r_m + i s_m$. We define
\begin{equation} \label{eq:SyMo:14}
	 \Phi = [r_1,\dots, r_k], \quad \Psi = [s_1,\dots, s_k].
\end{equation}
One can easily check that (\ref{eq:SyMo:12}) is satisfied {\edit since the matrix of singular vectors is unitary}. It is shown in \cite{Peng:2014di} that among all symplectic bases of the form (\ref{eq:SyMo:11}) the complex SVD minimizes the projection error.

\subsubsection{The Greedy Approach to Symplectic Basis Generation} \label{Chap:Symo.PrSy:3} Greedy generation of the reduced basis is an iterative procedure which, in each iteration, adds the two best possible basis vectors to the symplectic basis to enhance overall accuracy. In contrast to the cotangent lift and the complex SVD methods, the greedy approach does not require the symplectic basis to have a specific structure. This typically results in a more compact basis and/or more accurate reduced systems. For parametric problems, the greedy approach only requires one numerical solution to be computed per iteration hence saving substantial computational cost in the offline stage. 

{\edit The orthonormalization step is an essential step in most greedy approaches for basis generation in the context of model reduction \cite{Anonymous:2016wl,Quarteroni:2016wi}. However common orthonormalization processes, e.g. the QR method, destroy the symplectic structure of the original system \cite{BunseGerstner:1986dg}. Here we use a variation of the QR method known as the SR \cite{Salam2014} method which is based on the symplectic Gram-Schmidt method and yields a symplectic basis. 
}

{\edit
As discussed in Section \ref{chap:Hasy:1}, any finite dimensional symplectic linear vector space has a symplectic basis that satisfies conditions (\ref{eq:Hasy:4}). Further, Theorem \ref{theorem:Hasy:1.6} provides an iterative process for constructing an orthosymplectic basis \cite{Matsuo:2014wl,Salam2014}. To briefly describe the SR method, suppose that an orthosymplectic basis
\begin{equation} \label{eq:SyMo:14.1}
	A_{2k}=\{ e_1 , \dots , e_k \} \cup \{ \mathbb J_{2n}^T e_1 , \dots , \mathbb J_{2n}^T e_k \},
\end{equation}
and a vector $z\not \in \text{span}(A_{2k})$ is provided. We aim to symplectically orthogonalize ($\mathbb J_{2n}$-orthogonalize) $z$ with respect to $A_{2k}$ and seek $\alpha_1,\dots,\alpha_k,\beta_1,\dots,\beta_k \in \mathbb R$ such that
\begin{equation} \label{eq:SyMo:14.2}
	\Omega\left(z + \sum_{i=1}^k \alpha_i e_i + \sum_{i=1}^k \beta_i \mathbb J_{2n}^Te_i , \sum_{i=1}^k \bar{\alpha}_i e_i + \sum_{i=1}^k \bar{\beta}_i \mathbb J_{2n}^Te_i \right) = 0,
\end{equation}
for all possible $\bar{\alpha}_1,\dots,\bar{\alpha}_k,\bar{\beta}_1,\dots,\bar{\beta}_k \in \mathbb R$. It is easily seen that the unique solution is 
\begin{equation} \label{eq:SyMo:14.3}
	\alpha_i = - \Omega(z,\mathbb J_{2n}^Te_i), \quad \beta_i = \Omega(z,e_i),
\end{equation}
for $i=1,\dots,k$. Now define the modified vectors as
\begin{equation} \label{eq:SyMo:14.4}
	\tilde z = z - \sum_{i=1}^k \Omega(z,\mathbb J_{2n}^Te_i) e_i + \sum_{i=1}^k \Omega(z,e_i) \mathbb J_{2n}^Te_i.
\end{equation}
If we introduce $e_{k+1} = \tilde z / \| \tilde z \|_2$, it is easily checked that $e_{k+1}$ is also orthogonal to $A_{2k}$ with respect to the classical inner product. Therefore span$\{e_1,\dots,e_{k+1}\}$ forms a Lagrangian subspace and according to Theorem \ref{theorem:Hasy:1.6} the basis $A_{2k+2}= A_{2k}\cup \{ e_{k+1} , \mathbb J_{2n}^T e_{k+1} \}$ forms an orthosymplectic basis.

Note that the $SR$ method can be replaced with backward stable routines such as the isotropic Arnoldi or the isotropic Lanczos methods \cite{Mehrmann:2000dv}.
}

The key element of the greedy algorithm is the availability of an error function which evaluates the error associated with the model reduction \cite{Anonymous:2016wl}. In the framework of symplectic model reduction, one possible candidate is the error in the Hamiltonian (\ref{eq:SyMo:5}). Correctly approximating symplectic systems relies on preservation of the Hamiltonian, hence the error in the Hamiltonian {\edit arises as a} a natural choice. Moreover, since the error in the Hamiltonian depends on the initial condition and the reduced symplectic basis, evaluation of the error does not require the time integration of the full system. 

Suppose that a $2k$-dimensional {\edit orthosymplectic basis (\ref{eq:SyMo:14.1})} is generated at the $k$-th step of the greedy method and we seek to enrich it by two additional vectors. Using the error in the Hamiltonian (\ref{eq:SyMo:7}) we search the parameter space to identify the value that maximizes the error in the Hamiltonian
\begin{equation} \label{eq:SyMo:14.5}
	\omega_{k+1} := \underset{\omega\in \Gamma}{\text{argmax }}\Delta H(\omega).
\end{equation}
The goal is to approximate the Hamiltonian function as well as possible. 

We then propagate (\ref{eq:Hasy:7}) in time to produce trajectory snapshots 
\begin{equation}
	S=\{ \mathbf z(t_i,\omega_{k+1}) | i = 1,\dots,M \}.
\end{equation} 
The next basis vector is the snapshot that maximises the projection error (\ref{eq:SyMo:8})
{\edit 
\begin{equation} \label{eq:SyMo:14.6}
	z := \underset{s\in S}{\text{argmax }} \| s - A_{2k}{A_{2k}}^+s \|.
\end{equation}
}
Finally, we update the basis as
{\edit
\begin{equation} \label{eq:SyMo:14.7}
	e_{k+1} = \tilde z, \quad A_{2k+1} = A_{2k}\cup \{ e_{k+1} , \mathbb J_{2n}^Te_{k+1} \},
\end{equation}
}
where $\tilde z$ is the vector obtained {\edit after} applying the symplectic Gram-Schmidt process to $z$. 

Since the maximization over the entire parameter space $\Gamma$ is impossible, we discretize the parameter set into a grid with $N$ points: $\Gamma_N = \{ \omega_1,\dots,\omega_N\}$. However, since the selection of parameters only require the evaluation of the error in the Hamiltonian and not time integration of the original system, then $\Gamma_N$ can be chosen {\edit to be} very rich.

We summarize the greedy algorithm for the generation of a symplectic basis in Algorithm \ref{alg:SyMo:3}.




\begin{algorithm} 
\caption{The greedy algorithm for generation of a symplectic basis} \label{alg:SyMo:3}
{\bf Input:} Tolerated loss in the Hamiltonian $\delta$, parameter set $\Gamma_N = \{\omega_1,\dots,\omega_N\}$, initial condition $\mathbf z_0(\omega)$
\begin{enumerate}
\item $\omega^* \leftarrow \omega_1$
\item $e_1 \leftarrow \mathbf z_0(\omega^*)$
\item $A \leftarrow [e_1,-\mathbb J^T_{2n}e_1]$
\item $k \leftarrow 1$
\item \textbf{while} $\Delta H(\omega) > \delta$ for all $\omega \in \Omega_N$
\item \hspace{0.5cm} $w^* \leftarrow$ $\underset{\omega\in \Omega_N}{\text{argmax }}\Delta H(\omega)$
\item \hspace{0.5cm} Compute trajectory snapshots $S=\{ \mathbf z(t_i,\omega^*) | i = 1,\dots,M \}$
\item \hspace{0.5cm} $\mathbf z^* \leftarrow$ $\underset{s\in S}{\text{argmax }} \| s - AA^+s \|$
\item \hspace{0.5cm} Apply symplectic Gram-Schmidt on $\mathbf z^*$
\item \hspace{0.5cm} $e_{k+1} \leftarrow \mathbf z^*/ \| \mathbf  z^*\|$
\item \hspace{0.5cm} $A \leftarrow [e_1,\dots ,e_{k+1} , \mathbb J^T_{2n}e_1,\dots,\mathbb J^T_{2n}e_{k+1}]$
\item \hspace{0.5cm} $k \leftarrow k+1$
\item \textbf{end while}
\end{enumerate}
\vspace{0.5cm}
{\bf Output:} Symplectic basis $A$.
\end{algorithm}

%The convergence and convergence rate of the greedy algorithm with respect to Kolmogorov $n$-width is investigated in Section \ref{chap:SyMo.PrSy:3}. We finish this section by stating the advantage of using the greedy algorithm over the POD-based methods:
%\begin{itemize}
%\item Lack of a symplectic linear algebra toolbox for solving the minimization in (\ref{eq:SyMo:8}) requires a possibly very expensive nonlinear optimizations to recover near optimal symplectic bases. Alternative approaches for finding solutions to (\ref{eq:SyMo:8}) requires assumptions on the structure of the symplectic basis. The greedy algorithm on the other hand, is a cheap alternative to the nonlinear optimization and it allows flexibility in terms of the structure.
%\item The greedy method reduces the total cost of the offline stage by solving the original system (\ref{eq:Hasy:7}) only $k$ times to obtain a reduced system of size $2k$, unlike POD-based methods that require time integration of (\ref{eq:Hasy:7}) for the entire parameter space.
%\item If enrichment of the symplectic basis is required, the greedy algorithm can add new basis vectors. The POD-based methods, are bound to re-compute the basis for the refined parameter set.
%\item POD-based methods, e.g., the cotangent lift and the complex SVD, require performing SVD decompositions on possibly very large matrices. The greedy algorithm avoids this.
%\end{itemize}






\subsubsection{Convergence of the Greedy Method} \label{chap:SyMo.PrSy:3}

To show convergence of the greedy method we {\edit consider} a slightly different version based on the projection error. The error in the Hamiltonian is then introduced as a cheap surrogate to the projection error to accelerate the parameter selection.

Suppose that we are given a compact subset $S$ of $\mathbb R^{2n}$. Our intention is to find a set of vectors $A=\{e_1,\dots,e_k,f_1,\dots,f_k\}$ such that $A$ forms {\edit an orthosymplectic} basis and any $s\in S$ is well approximated by elements of the subspace span$(A)$. The modified greedy method for generating basis vectors $e_i$ and $f_i$ is as follows. In the initial step we pick $e_1$ such that $\edit \|e_1\|_2 = \max_{s\in S} \|s\|_2$. Then define $f_1 = \mathbb{J}_{2n}^T e_1$. It is easy to check that the span of $A_2 = \{e_1,f_1\}$ is {\edit orthosymplectic}, so $A_2$ is the first subspace that approximates elements of $S$. In the $k$-th step of the greedy method, suppose we have a basis $A_{2k} = \{ e_1,\dots, e_k , f_1,\dots ,f_k \}$. We define $P_{2k}$ to be a symplectic projection operator that projects elements of $S$ onto span$(A_{2k})$ and define
\begin{equation} \label{eq:new1}
	\sigma_{2k}(s) := \|s-P_{2k}(s)\|_2,
\end{equation}
as the projection error. Moreover we denote by $\sigma_{2k}$ the maximum approximation error of $S$ using elements in span$(A_{2k})$ as
\begin{equation} \label{eq:new2}
	\sigma_{2k} := \max_{s\in S} \sigma_{2k}(s).
\end{equation}
The next set of basis vectors in the greedy selection are
\begin{equation} \label{eq:new3}
	e_{k+1} := \underset{s\in S}{\text{argmax }}\sigma_{2k}(s), \quad f_{k+1} := \mathbb{J}^T e_{k+1}.
\end{equation}
We emphasisze that the sequence of basis vectors generated by the greedy is generally not unique. 

To estimate the quality of the reduced subspace, it is natural to compare it with the best possible $2k$-dimensional subspace in the sense of the minimum projection (not necessary symplectic) error. For this we introduce the Kolmogorov $n$-width \cite{Kolmogoroff:1936fj,Pinkus:1985vy}.

\begin{definition}
Let $S$ be a subset of $\mathbb R^{m}$ and $Y_n$, $n\leq m$, be a general $n$-dimensional subspace of $\mathbb R^{m}$. The angle between $S$ and $Y_n$ is given by
\begin{equation} \label{eq:new4}
	E(S,Y_n) := \sup_{s\in S} \inf_{y\in Y_n} \|s-y\|_2.
\end{equation}
The Kolmogorov $n$-width of $S$ in $\mathbb R^m$ is given by
\begin{equation} \label{eq:new5}
	d_{n}(S,\mathbb{R}^m) := \inf_{Y_n} E(S,Y_n) = \inf_{Y_n} \sup_{s\in S} \inf_{y\in Y_n} \|s-y\|_2
\end{equation}
\end{definition}

For a given subspace $Y_n$, the angle between $S$ and $Y_n$ measures the worst possible projection error of elements in $S$ onto $Y_n$. Hence the Kolmogorov $n$-width quantifies how well $S$ can be approximated by {\edit an} $n$-dimensional subspace. 

We seek to show that the decay of $\sigma_{2k}$, obtained by the greedy algorithm, has the same rate as of $d_{2k}(S)$, i.e., the greedy method provides the best possible accuracy attained by a $2k$-dimensional subspace.

We start by symplectifying the vectors provided by the greedy algorithm as
\begin{equation} \label{eq:new6}
\begin{aligned}
	& \xi_1 = e_i, & \bar{\xi}_1 = \mathbb{J}_{2n}^T \xi_1, &\\
	& \xi_i = e_i - P_{2(i-1)} (e_i), & \bar{\xi}_i = \mathbb{J}_{2n}^T, \xi_i &\quad i = 2,3,\dots
\end{aligned}
\end{equation}
The projection of a vector $s\in S$ onto span$(A_{2k})$ can be written using the symplectic basis as
\begin{equation} \label{eq:new7}
	P_{2k}(s) = \sum_{i=1}^k \left( \alpha_i(s) \xi_i + \bar{\alpha}_i(s) \bar{\xi}_i \right),
\end{equation}
where $\alpha_i(s)$ and $\bar{\alpha}_i(s)$ for $i=1,\dots,k$ are the expansion coefficients
\begin{equation} \label{eq:new8}
	\alpha_i(s) = - \frac{\Omega(\bar{\xi}_i,s)}{\Omega(\xi_i,\bar{\xi}_i)}, \quad \bar{\alpha}_i(s) = \frac{\Omega(\xi_i,s)}{\Omega(\xi_i,\bar{\xi}_i)},
\end{equation}
for any $s\in S$. Since $\bar{\xi}_i$ is symplectic to span$(A_{2(k-1)})$ we have
\begin{equation} \label{eq:new9}
\begin{aligned}
	|\alpha_i(s)| = \frac{|\Omega(\bar{\xi}_i,s)|}{|\Omega(\xi_i,\bar{\xi}_i)|} = \frac{|\Omega( \bar{\xi}_i, s - P_{2(k-1)}(s))|}{|\Omega(\xi_i,\bar{\xi}_i)|}  &\leq \frac{\|\bar{\xi}_i\|_2 \| s - P_{2(k-1)}(s) \|_2}{ \|\xi_i\|_2 \|\bar{\xi}_i\|_2 } \\
	&= \frac{\| s - P_{2(k-1)}(s) \|_2}{\| e_i - P_{2(k-1)}(e_i) \|_2} \leq 1.
\end{aligned}
\end{equation}
Here, we use the fact that $\edit |\Omega(\xi_i,\bar{\xi}_i)| = \| \xi_i \|^2_2 = \|\bar{\xi}_i\|^2_2$ with the last inequality following from the greedy algorithm which maximizes $e_i$. Similarly we deduce that $|\bar{\alpha}_i(s)|\leq 1$.

We write
\begin{equation} \label{eq:new10}
\begin{aligned}
	\xi_j = \sum_{i=1}^j \left( \mu_i^j e_i + \gamma_i^j f_i \right), \quad \bar{\xi}_j = \sum_{i=1}^j \left( \lambda_i^j e_i + \eta_i^j f_i,  \right), \quad j=1,2,\dots
\end{aligned}
\end{equation}
with
\begin{equation} \label{eq:new11}
\begin{aligned}
	&\mu^j_j = 1, \quad \gamma^j_j = 0, \\
	&\mu_i^j = \sum_{l=i}^{j-1}\left( - \alpha_l(f_j) \mu_i^l  + \bar{\alpha}_l(f_j) \gamma_i^l \right), \quad\gamma_i^j = \sum_{l=i}^{j-1}\left( - \alpha_l(f_j) \gamma_i^l  + \bar{\alpha}_l(f_j) \mu_i^l \right), \\
	&\lambda^j_i = - \gamma ^j_i, \quad \eta^j_i = \mu^j_i,
\end{aligned}
\end{equation}
for $j=2,3,\dots$. By induction and using the bound in (\ref{eq:new9}) we deduce that
\begin{equation} \label{eq:new12}
	\mu^j_i,\gamma^j_i,\lambda^j_i,\eta^j_i \leq 3^{j-i}, \quad \text{for } j\geq i.
\end{equation}
Now let $2k$ be the dimension of the desired reduced space. Looking at the definition of Kolmogorov $n$-width we observe that for any $\theta > 1$ we can find a subspace $Y_{2k}$ such that $E(S,Y_{2k}) \leq \theta d_{2k}(S,\mathbb R^n)$. Hence we can find vectors $v_1,\dots,v_k,u_1,\dots,u_k\in Y_{2k}$ such that
\begin{equation} \label{eq:new13}
\begin{aligned}
	& \|e_i - v_i\|_2 \leq \theta d_{2k}(S,\mathbb R^n), \\
	& \|f_i - u_i\|_2 \leq \theta d_{2k}(S,\mathbb R^n).
\end{aligned}
\end{equation}
Now we construct a set of $2(k+1)$ new vectors
\begin{equation} \label{eq:new14}
\begin{aligned}
	& \zeta_j = \sum_{i=1}^{k+1} \mu_i^j v_i + \gamma^j_i u_i,\quad \bar{\zeta}_j = \sum_{i=1}^{k+1} \lambda_i^j v_i + \eta^j_i u_i.
\end{aligned}
\end{equation}
for $j = 1,\dots,k+1$. Note that since $u_i$ and $v_i$ belong to $Y_{2k}$ so does their linear combination including all $\zeta_j$ and $\bar{\zeta}_j$. We can use the inequality (\ref{eq:new12}) to write
\begin{equation}
	\| \xi_i - \zeta_i \|_2 \leq 3^i \theta d_{2k}(S,\mathbb R^n),\quad \| \bar{\xi}_i - \bar{\zeta}_i \|_2 \leq 3^i \theta d_{2k}(S,\mathbb R^n).
\end{equation}
Moreover since $Y_{2k}$ is of dimension $2k$ we find $\kappa_i$, $i=1,\dots,2(k+1)$ such that
\begin{equation} \label{eq:new15}
	\sum_{i=1}^{2(k+1)} \kappa_i^2 = 1, \quad\sum_{i=1}^{k+1} \kappa_i \zeta_i + \sum_{i=1}^{k+1} \kappa_{i+k+1} \bar{\zeta}_i = 0.
\end{equation}
We have
\begin{equation} \label{eq:new17}
\begin{aligned}
	\left\| \sum_{i=1}^{k+1} \kappa_i \xi_i + \sum_{i=1}^{k+1} \kappa_{i+k+1} \bar{\xi}_i \right\|_2 &= \left\| \sum_{i=1}^{k+1} \kappa_i (\xi_i - \zeta_i) + \sum_{i=1}^{k+1} \kappa_{i+k+1} (\bar{\xi}_i-\bar{\zeta}_i) \right\|_2 \\
	&\leq 2\cdot 3^{k+1} \sqrt{2(k+1)} \theta d_{2k}(S,\mathbb R^n).
\end{aligned}
\end{equation}
We know there exists $1 \leq j\leq 2k+2$ such that $\kappa_j > 1/\sqrt{2(k+1)}$. Without loss of generality let us assume that $j\leq k+1$. This yields
\begin{equation} \label{eq:new18}
	\left\| \xi_j +  \kappa_j^{-1} \sum_{i=1,i\neq j}^{k+1} \kappa_i \xi_i + \kappa_j^{-1}\sum_{i=1}^{k+1} \kappa_{i+k+1} \bar{\xi}_i \right\|_2 \leq 4\cdot 3^{k+1} (k+1) \theta d_{2k}(S,\mathbb R^n).
\end{equation}
Define $c = \kappa_j^{-1} \sum_{i=1,i\neq j}^{k+1} \kappa_i \xi_i + \kappa_j^{-1}\sum_{i=1}^{k+1} \kappa_{i+k+1} \bar{\xi}_i$ and note that $\mathbb{J}_{2n}^T c$ is symplectic to $\xi_j$. We recover
\begin{equation} \label{eq:new19}
\begin{aligned}
	\| \xi_j \|_2 &\leq \| \xi_j \|_2 + \| c \|_2 = \Omega(\xi_j,\mathbb{J}_{2n}^T \xi_j) + \Omega(c,\mathbb{J}_{2n}^T c) \\
	       &= \Omega(\xi_j,\mathbb{J}_{2n}^T \xi_j) + \Omega(c,\mathbb{J}_{2n}^T c) + \Omega(\xi_j,\mathbb{J}_{2n}^T c) + \Omega(c,\mathbb{J}_{2n}^T \xi_j) \\
       	&= \Omega(\xi_j + c, \mathbb{J}^T_{2n} (\xi_j + c)) = \| \xi_j + c \|_2	
\end{aligned}
\end{equation}
Combining this with (\ref{eq:new18}) yields
\begin{equation} \label{eq:new20}
	\| \xi_j \|_2 \leq 4\cdot 3^{k+1} (k+1) \theta d_{2k}(S,\mathbb R^n).
\end{equation}
Finally using the definition of $\xi_j$ for all $s\in S$ we have
\begin{equation} \label{eq:new21}
	\| s - P_{2(j-1)}(s) \|_2 \leq \| f_j - P_{2(j-1)}(f_j) \|_2 = \|\xi_j \|_2 \leq 4\cdot 3^{k+1} (k+1) \theta d_{2k}(S,\mathbb R^n)
\end{equation}
Hence, for any given $\lambda > 1$
\begin{equation} \label{eq:new22}
	\| s - P_{2k}(s) \|_2 \leq \| s - P_{2(j-1)}(s) \|_2 \leq 4\cdot 3^{k+1} (k+1) \theta d_{2k}(S,\mathbb R^n).
\end{equation}
This establishes the following theorem.
\begin{theorem} \label{theorem:SyMo:2}
	Let $S$ be a compact subset of $\mathbb{R}^{2n}$ with exponentially small Kolmogorov $n$-width $\edit d_{k}\leq c\exp(-\alpha k)$ with $\alpha > \log3$. Then there exists $\beta>0$ such that the symplectic subspaces $A_{2k}$ generated by the greedy algorithm provide exponential approximation properties such that
\begin{equation} \label{eq:new23}
	\| s - P_{2k}(s) \|_2 \leq C \exp(-\beta k)
\end{equation}
for all $s\in S$ and some $C>0$.
\end{theorem}

%{\edit
%\begin{theorem} \label{theorem:SyMo:3}
%	The basis $A_{2k}$ generated by the greedy algorithm is orthosymplectic.
%\end{theorem}
%\begin{proof}
%	Direct result of theorem \ref{theorem:Hasy:1.6}.
%\end{proof}
%}

\subsection{Symplectic Discrete Empirical Interpolation Method (SDEIM)} Consider the Hamiltonian system (\ref{eq:Hasy:7}) and its reduced system (\ref{eq:SyMo:4}) equipped with a symplectic transformation $A$. One can split the Hamiltonian function $H = H_1 + H_2$ such that $\nabla H_1 = L\mathbf z$ and $\nabla H_2 = \mathbf g(\mathbf z)$, where $L$ is a constant matrix in $\mathbb R^{n\times n}$ and $\mathbf g$ is a nonlinear function. The reduced system takes the form
\begin{equation} \label{eq:new24}
	\frac{d}{dt} \mathbf y = \underbrace{A^+ \mathbb J_{2n} L A}_{\tilde L} \mathbf y + A^+ \mathbb J_{2n} \mathbf g(A\mathbf y)
\end{equation}
As discussed in Section \ref{chap:MoOr.DEIM:1}, the complexity of evaluating the nonlinear term still depends on $n$, the size of the original system. To overcome this computational bottleneck we use the DEIM approximation for evaluating the nonlinear function $\mathbf g$ as
\begin{equation} \label{eq:new25}
	\frac{d}{dt} \mathbf y = \tilde L \mathbf y + \underbrace{ A^+ \mathbb J_{2n} V (P^TV)^{-1} P^T \mathbf g(A\mathbf y) }_{\tilde N(\mathbf y)}
\end{equation}
For a general choice of $V$ the system (\ref{eq:new25}) is not guaranteed to be a Hamiltonian system, impacting long time accuracy and stability. However, we can guarantee that (\ref{eq:new25}) is a Hamiltonian system by choosing $V=(A^+)^T$. To {\edit see this}, we note that the system (\ref{eq:new25}) is a Hamiltonian system if and only if $\tilde N(\mathbf y) = \mathbb J_{2k} \nabla_{\mathbf y} \mathbf g(\mathbf y)$. Also we have 
\begin{equation} \label{eq:new26}
	\mathbf g(A\mathbf y) = \nabla_{\mathbf z} H_2(\mathbf z) = (A^+)^T \nabla_{\mathbf y} H_2(A \mathbf y),
\end{equation}
where the chain rule is used for the second equality. Substituting this into $\tilde N$ we obtain
\begin{equation} \label{eq:new27}
	\tilde N(\mathbf y)= A^+ \mathbb J_{2n} V (P^TV)^{-1} P^T  (A^+)^T \nabla_{\mathbf y} H_2(A \mathbf y).
\end{equation}
Taking $V = (A^+)^T$ yields
\begin{equation} \label{eq:new28}
	\tilde N(\mathbf y) = A^+ \mathbb J_{2n}(A^+)^T \nabla_{\mathbf y} H_2(A \mathbf y) = \mathbb J_{2k} \nabla_{\mathbf y} H_2(A \mathbf y),
\end{equation}
since $(A^+)^T$ is a symplectic matrix. Hence, $V = (A^+)^T$ is a sufficient condition for (\ref{eq:new25}) to {\edit be Hamiltonian}. 

Regarding the construction of the projection space, suppose that we have already constructed a symplectic basis $A=\{ e_1,\dots , e_k,f_1,\dots f_k \}$ using the greedy algorithm. Note that $(A^+)^T$ is a symplectic basis and $(A^+)^+=A$. Thus, we can move between these two symplectic bases by simply using the transpose operator and the symplectic inverse operator. Let $S_{\mathbf g} = \{ \mathbf g (\mathbf x(t_i,\omega_j)) \}$ with $i = 1,\dots,M$ and $ j = 1 ,\dots,N$ be the nonlinear snapshots that were gathered in the greedy algorithm. We then form $(A^+)^T = \{ e'_1,\dots, e'_k,f'_1,\dots,f'_k\}$ and use a greedy approach to add new basis vectors to $(A^+)^T$. At the $i$-th iteration of the symplectic DEIM, we use $(A^+)^T$ to approximate elements in $S_{\mathbf g}$ and choose the vector that maximizes the error as the next basis vector 
\begin{equation}
	s^* := \underset{s \in S_{\mathbf g}}{\text{argmax }}\| s - (A^+)^T A^+ s \|_2.	
\end{equation}
After applying the symplectic Gram-Schmidt on $s^*$, we update $(A^+)^T$ as
\begin{equation}
\begin{aligned}
	e'_{k+i+1} = \frac{s^*}{\| s^* \|_2},\quad f'_{k+i+1} = \mathbb J_{2n}^T e'_{k+i+1}.
\end{aligned}
\end{equation}
Finally when $(A^+)^T$ approximates elements $S_{\mathbf g}$ with the desired accuracy, we transpose and symplectically invert $(A^+)^T$ to obtain $A$. We summarize the symplectic DEIM algorithm in Algorithm \ref{alg:SyMo:4}.



\begin{algorithm} 
\caption{Symplectic Discrete Empirical Interpolation Method} \label{alg:SyMo:4}
{\bf Input:} Symplectic basis $A=\{ e_1,\dots,e_k,f_1,\dots,f_k \}$, nonlinear snapshots $S_{\mathbf g} = \{ \mathbf g(\mathbf x(t_i,\omega_j)) \}$ and tolerance $\delta$
\begin{enumerate}
\item Compute $(A^+)^T = \{ e'_1,\dots,e'_k,f'_1,\dots,f'_k \}$
\item $i \leftarrow 1$
\item \textbf{while} max$\| s - (A^+)^T A^+s \| > \delta$ for all $s\in S_{\mathbf g}$
\item \hspace{0.5cm} $s^* \leftarrow \underset{s \in S_{\mathbf g}}{\text{argmax }}\| s - (A^+)^T A^+ s \|$
\item \hspace{0.5cm} Apply symplectic Gram-Schmidt on $s^*$
\item \hspace{0.5cm} $e'_{k+i} = s^* / \| s^* \|$
\item \hspace{0.5cm} $f'_{k+i} = \mathbb J_{2n} e'_{k+i}$
\item \hspace{0.5cm} $(A^+)^T \leftarrow [e'_1,\dots,e'_{k+i},f'_1,\dots,f'_{k+i}]$
\item \hspace{0.5cm} $i\leftarrow i+1$
\item \textbf{end while}
\item take transpose and symplectic inverse of $(A^+)^T$
\end{enumerate}
\vspace{0.5cm}
{\bf Output:} Symplectic basis $A$ that guarantees a Hamiltonian reduced system.
\end{algorithm}

When using an implicit time integration scheme we face inefficiencies when evaluating the Jacobian of nonlinear terms, as discussed in Section \ref{chap:MoOr.DEIM:1}. We recall that the key to fast approximation of the Jacobian is that the interpolating index matrix $P$, obtained in the DEIM approximation, commutes with the nonlinear function. Nonlinear terms in Hamiltonian systems often take the from
\begin{equation}
	\mathbf g (\mathbf z) = \mathbf g (\mathbf q,\mathbf p) = 
	\begin{pmatrix}
		g_1(q_1,p_1) \\
		g_2(q_2,p_2) \\
		\vdots \\
		g_{2n}(q_{n},p_{n})
	\end{pmatrix}.
\end{equation}
Thus, the interpolating index matrix, obtained by Algorithm \ref{alg:MoOr:1} does not necessarily commute with the function $\mathbf g$. To overcome this, when index $\mathfrak p_i$ with $\mathfrak p_i\leq n$ or $\mathfrak p_i>n$ is chosen in Algorithm \ref{alg:MoOr:1} we also include $\mathfrak p_i + n$ or $\mathfrak p_i-n$, respectively. {\edit Simple calculations verifies that $\mathbf g$ and $P$ commute.}
